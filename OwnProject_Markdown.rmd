---
title: "Adult Census Income"
author: "Abraham Verde"
date: "March/8/2022"
output:
  pdf_document: default
  html_document: default
---

## INTRODUCTION 
For this project I used a dataset that contains an Adult Census of USA in 1994. This dataset has 15 variables and a field that tell us if person's income is lower o higher than 50k usd per year.

In this project, I will perform two machine learning algorithms to predict the income in a test set. This algorithms will be GLM (generalized linear model) and Random Forest.

The data set used in this project I downloaded from kaggel website via https://www.kaggle.com/ and is also available to download from my github account https://github.com/abrahamverde/adult_census/raw/master/adult.csv.


```{r error=FALSE, warning=FALSE}
if(!require(readr)) install.packages("readr", repos = "http://cran.us.r-project.org")
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(ggplot2)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(dplyr)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(ranger)) install.packages("ranger", repos = "http://cran.us.r-project.org")



library(readr)
library(tidyverse)
library(caret)
library(data.table)
library(ggplot2)
library(dplyr)
library(ranger)


```

## ANALYSIS

The first step is getting the data from csv file.

#### LOAD CSV DATA SET FROM MY GITHUB ACCOUNT

```{r error=FALSE, warning=FALSE}

datasetURL <- "https://github.com/abrahamverde/adult_census/raw/master/adult.csv"
rawDataSet <- read.csv(datasetURL)

```



Once I loaded the dataset, I started to do some data exploration.

```{r}
#GET COLUMNS NAME
names(rawDataSet)

#GET A LITTLE SAMPLE DATA
head(rawDataSet, 15)

#IT'S IMPORTANT TO KNOW THE LENGHT OF DATASET
nrow(rawDataSet)

#PEOPLE WITH THEIR INCOME
peopleIncome <- table(rawDataSet$income)
peopleIncome

#INCOME RATE
peopleIncome_rate <- prop.table(peopleIncome)
peopleIncome_rate
```



In the next graphs we can easily see the diference between income values by people. Almost the 75% of the people earn an income lower than 50k per year.

```{r}
#GRAPH OF QTY
barplot(peopleIncome,main = 'INCOME AND QUANTITY OF PEOPLE',ylab ='People')

#GRAPH OF RATE
barplot(peopleIncome_rate,main = 'INCOME AND QUANTITY OF PEOPLE - RATE',ylab ='Rate')
```



In the next graph we can see the income of the people by age. We can conclude that the higher income is between 30 to 50 years old.

```{r}
#INCOME BY AGE
ggplot(rawDataSet) + aes(x=age, group=income, fill=income) + 
  geom_histogram(binwidth=1, color='black')+
  labs(x="Age",y="Count",title = "INCOME BY AGE")
```


By the same way, we can see the higher income belong to male population.
```{r}
ggplot(data=rawDataSet,
       aes(age,group=sex,fill=sex))+
  geom_histogram(binwidth=1, color='red')+ ggtitle('INCOME BY SEX')
```


The exploration data makes me realize there are some really relevant fields for the prediction. So, I'm going to select just the relevants ones.


```{r}
cleanData <- rawDataSet %>% select(income, sex, age)
```

Some data exploration over the clean data dataframe.

```{r}
#PREVIEW
head(cleanData, 50)


#CHECK FOR NA
colSums(is.na(cleanData))


#CHECK STRUCTURE
str(cleanData)

```


I realized the characters inside income field could be a problem in the next steps. I decided to change name to these factors.
```{r}
levels(cleanData$income)<-c("lower50", "higher50")
str(cleanData)

```


##### CREATE DATA PARTITION

For this prediction, I'm using the 70% to train_set and 30% to test_set.

```{r}
set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
test_index <- createDataPartition(cleanData$income, times = 1, p = 0.3, list = FALSE)
train_set <- cleanData[-test_index,]
test_set <- cleanData[test_index,]


#EXPLORING DATA PARTITION
nrow(train_set)
nrow(test_set)
head(train_set, 15)
head(test_set, 15)
table(train_set$income)
```

##### FIT GLM MODEL

Before try to fit the model, I setup the Train Control Object. This object will "control" the glm train.

```{r}
trainControlObject <- trainControl(method="cv", number = 10, classProbs = TRUE, summaryFunction = twoClassSummary)

#Here I try to fit the model. This process could take a while depending on your computer.
fit_glm <- train(income~., data = train_set, trControl=trainControlObject, family = binomial, method = "glm", metric="ROC")

```


So far, I got a Logistic Regression model. The randomForest approach is a very popular approach therefore I dediced fit a model using randomForest and show both results (glm and random forest approach.).

```{r}
#RANDOM FOREST USING RANGER PACKAGE (THIS IS FASTER THAN OLDER PACKAGE "randomForest" )
fit_randomforest <- train(income~., data = train_set, method = "ranger", metric="ROC",num.trees=50,
                          trControl=trainControlObject)
```

## RESULTS

Finally, I have results in both approach.
```{r}
#GET RESULTS USING RESAMPLES FUNCTION
Results <- resamples(list(LG=fit_glm, RFOREST=fit_randomforest))

#SHOW SOME SUMMARY
summary(Results)
```


## CONCLUSIONS
After running both model, and taking account their accuracy. I can conclude both techniques give close results but random forest is a little bit better accuracy when is compared with GLM.
